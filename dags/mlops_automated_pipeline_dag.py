import os
import pendulum
import requests
import json

from airflow.models.dag import DAG
from airflow.operators.python import PythonOperator
from airflow.sensors.python import PythonSensor
from airflow.exceptions import AirflowException
from datetime import timedelta


# ì„œë²„1 IP ì£¼ì†Œ í™˜ê²½ ë³€ìˆ˜ ì½ê¸°
SERVER_1_IP_ADDRESS = os.getenv('SERVER_1_IP_ADDRESS')
if not SERVER_1_IP_ADDRESS:
    raise ValueError("Airflow í™˜ê²½ ë³€ìˆ˜ì— 'SERVER_1_IP_ADDRESS'ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

SERVER1_API_BASE = f"http://{SERVER_1_IP_ADDRESS}:8000"


def _check_api_health():
    """ì„œë²„1 FastAPIì˜ /health ì—”ë“œí¬ì¸íŠ¸ë¥¼ í˜¸ì¶œí•˜ì—¬ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""
    try:
        url = f"{SERVER1_API_BASE}/health"
        response = requests.get(url, timeout=60) # íƒ€ì„ì•„ì›ƒ 60ì´ˆ(1ë¶„)
        response.raise_for_status() # 2xx ìƒíƒœ ì½”ë“œê°€ ì•„ë‹ˆë©´ HTTPError ë°œìƒ
        
        health_status = response.json().get("status")
        if health_status == "ok":
            print(f"âœ… API Health-check ì„±ê³µ: {response.json()}")
            return True
        else:
            print(f"ğŸš¨ API ìƒíƒœê°€ 'ok'ê°€ ì•„ë‹™ë‹ˆë‹¤: {response.json()}")
            return False
            
    except requests.exceptions.RequestException as e:
        print(f"ğŸš¨ API Health-check ì¤‘ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False

def _run_pipeline_task(endpoint: str, expected_result: str, payload: dict = None):
    """ì§€ì •ëœ FastAPI ì—”ë“œí¬ì¸íŠ¸ë¥¼ í˜¸ì¶œí•˜ê³  ê²°ê³¼ë¥¼ ê²€ì¦í•˜ëŠ” ê³µí†µ í•¨ìˆ˜ (ì˜ˆì™¸ ì²˜ë¦¬ ê°•í™”)"""
    url = f"{SERVER1_API_BASE}/{endpoint}"
    headers = {"Content-Type": "application/json"}
    
    print(f"ğŸš€ API í˜¸ì¶œ ì‹œì‘: {url}, í˜ì´ë¡œë“œ: {payload or 'ì—†ìŒ'}")
    
    try:
        if payload:
            response = requests.post(url, headers=headers, data=json.dumps(payload), timeout=600) # 10ë¶„ íƒ€ì„ì•„ì›ƒ
        else:
            # timeout=1800ì´ˆ(30ë¶„)ë¡œ ì„¤ì •í•˜ì—¬ ê¸´ ì‹¤í–‰ ì‹œê°„ í—ˆìš©
            response = requests.post(url, headers=headers, timeout=1800)
            
        # 1. HTTP ìƒíƒœ ì½”ë“œ ê²€ì‚¬
        response.raise_for_status()
        
        # 2. ì‘ë‹µ JSON ë‚´ìš© ê²€ì‚¬
        response_json = response.json()
        result = response_json.get("result")
        if result == expected_result:
            print(f"âœ… íƒœìŠ¤í¬ ì„±ê³µ: {response_json}")
            return response_json
        else:
            # ì„±ê³µ ì‘ë‹µ(200)ì„ ë°›ì•˜ì§€ë§Œ, ë‚´ìš©ì´ ê¸°ëŒ€ì™€ ë‹¤ë¥¸ ê²½ìš°
            error_message = f"ğŸš¨ íƒœìŠ¤í¬ ì‘ë‹µ ê²€ì¦ ì‹¤íŒ¨. ì˜ˆìƒ ê²°ê³¼: '{expected_result}', ì‹¤ì œ ì‘ë‹µ: {response_json}"
            print(error_message)
            raise AirflowException(error_message)
            
    except requests.exceptions.HTTPError as e:
        # 4xx, 5xx ì—ëŸ¬ ì²˜ë¦¬
        error_message = f"ğŸš¨ APIê°€ ì˜¤ë¥˜ ì‘ë‹µì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤ (HTTP Error). ìƒíƒœì½”ë“œ: {e.response.status_code}, ì‘ë‹µ: {e.response.text}"
        print(error_message)
        raise AirflowException(error_message)
        
    except requests.exceptions.RequestException as e:
        # ë„¤íŠ¸ì›Œí¬ ì—°ê²° ì˜¤ë¥˜, íƒ€ì„ì•„ì›ƒ ë“±
        error_message = f"ğŸš¨ API í˜¸ì¶œ ì¤‘ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë°œìƒ: {e}"
        print(error_message)
        raise AirflowException(error_message)
        
    except Exception as e:
        # JSON íŒŒì‹± ì‹¤íŒ¨ ë“± ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜
        error_message = f"ğŸš¨ íƒœìŠ¤í¬ ì‹¤í–‰ ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ ë°œìƒ: {e}"
        print(error_message)
        raise AirflowException(error_message)

default_args = {
    'owner': 'mlops-3xplusy-team',
    'depends_on_past': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

with DAG(
    dag_id="mlops-pipeline-gemini-dag",
    default_args=default_args,
    start_date=pendulum.datetime(2025, 1, 1, tz="Asia/Seoul"),
    schedule_interval='*/10 * * * *',
    catchup=False,
    tags=["mlops", "fastapi", "automated-pipeline"],
    doc_md="""
    ### MLOps ì „ì²´ íŒŒì´í”„ë¼ì¸ DAG (í™˜ê²½ ë³€ìˆ˜ ê¸°ë°˜)
    
    **í™˜ê²½ ë³€ìˆ˜**ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„œë²„1ì˜ API URLì„ ë™ì ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
    **PythonOperator**ë¥¼ ì‚¬ìš©í•˜ì—¬ FastAPI ì—”ë“œí¬ì¸íŠ¸ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
    
    1. **ë°ì´í„° ì¤€ë¹„**: TMDB ë°ì´í„° í¬ë¡¤ë§, ì „ì²˜ë¦¬, PostgreSQL/S3 ì €ì¥
    2. **ëª¨ë¸ í•™ìŠµ**: ë°ì´í„°ì…‹ ë¡œë“œ, MLflow ì—°ë™ í•™ìŠµ, ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ì €ì¥
    3. **ë°°ì¹˜ ì¶”ë¡ **: í•™ìŠµëœ ìµœì‹  ëª¨ë¸ë¡œ ë°°ì¹˜ ì¶”ë¡  ìˆ˜í–‰, ê²°ê³¼ ì €ì¥
    """,
) as dag:
    # 1. API ì„œë²„ ìƒíƒœ í™•ì¸ (PythonSensor)
    api_health_check = PythonSensor(
        task_id="check_api_health",
        python_callable=_check_api_health,
        poke_interval=10,
        timeout=60,
        mode="poke",
    )

    # 2. ë°ì´í„° ì¤€ë¹„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    run_prepare_data_task = PythonOperator(
        task_id="run_prepare_data",
        python_callable=_run_pipeline_task,
        op_kwargs={
            "endpoint": "run/prepare-data",
            "expected_result": "prepare-data finished",
        },
    )

    # 3. ëª¨ë¸ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    run_train_task = PythonOperator(
        task_id="run_train",
        python_callable=_run_pipeline_task,
        op_kwargs={
            "endpoint": "run/train",
            "expected_result": "train finished",
            "payload": {"model_name": "movie_predictor"},
        },
    )

    # 4. ëª¨ë¸ ë°°ì¹˜ ì¶”ë¡  íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    run_batch_inference_task = PythonOperator(
        task_id="run_batch_inference",
        python_callable=_run_pipeline_task,
        op_kwargs={
            "endpoint": "run/model-inference",
            "expected_result": "model-inference finished",
        },
    )

    # íƒœìŠ¤í¬ ì˜ì¡´ì„± ì„¤ì •
    api_health_check >> run_prepare_data_task >> run_train_task >> run_batch_inference_task